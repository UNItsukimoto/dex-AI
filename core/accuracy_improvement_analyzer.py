#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
äºˆæ¸¬ç²¾åº¦æ”¹å–„åˆ†æãƒ„ãƒ¼ãƒ«
æœªé”æˆæœŸé–“ã®è©³ç´°åˆ†æã¨æ”¹å–„ç­–ã®ç‰¹å®š
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from improved_prediction_system import ImprovedPredictionSystem

class AccuracyImprovementAnalyzer:
    """äºˆæ¸¬ç²¾åº¦æ”¹å–„åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.system = ImprovedPredictionSystem()
        self.results_dir = Path("results/accuracy_improvement")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # ç›®æ¨™æœªé”æˆæœŸé–“
        self.target_periods = ['current', '2025_06', '2025_05']
        self.current_accuracies = {
            'current': 0.4839,
            '2025_06': 0.4833,
            '2025_05': 0.4885
        }
        
    def analyze_underperforming_periods(self):
        """æœªé”æˆæœŸé–“ã®è©³ç´°åˆ†æ"""
        logger.info("æœªé”æˆæœŸé–“ã®è©³ç´°åˆ†æã‚’é–‹å§‹...")
        
        analysis_results = {}
        
        for period in self.target_periods:
            logger.info(f"\n=== {period} æœŸé–“ã®åˆ†æ ===")
            
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = self.system.load_period_data(period)
            if df is None or len(df) < 30:
                logger.warning(f"{period}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
                continue
            
            # åˆ†æå®Ÿè¡Œ
            period_analysis = self._analyze_single_period(period, df)
            analysis_results[period] = period_analysis
        
        return analysis_results
    
    def _analyze_single_period(self, period_name, df):
        """å˜ä¸€æœŸé–“ã®è©³ç´°åˆ†æ"""
        logger.info(f"{period_name} ã®åˆ†æé–‹å§‹ - ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}")
        
        # ç‰¹å¾´é‡ä½œæˆ
        df_features = self.system.create_advanced_features(df)
        
        # åŸºæœ¬çµ±è¨ˆ
        basic_stats = {
            'data_points': len(df),
            'feature_points': len(df_features),
            'data_loss_ratio': (len(df) - len(df_features)) / len(df),
            'price_volatility': df['close'].pct_change().std(),
            'price_trend': (df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0],
            'volume_stability': df['volume'].std() / df['volume'].mean() if df['volume'].mean() > 0 else 0
        }
        
        # ç‰¹å¾´é‡åˆ†æ
        exclude_cols = ['target', 'target_direction', 'open', 'high', 'low', 'close', 'volume']
        feature_cols = [col for col in df_features.columns if col not in exclude_cols]
        
        X = df_features[feature_cols].values
        y = df_features['target'].values
        
        # æ¬ æå€¤å‡¦ç†
        X = np.nan_to_num(X, nan=0, posinf=1e6, neginf=-1e6)
        
        # ç‰¹å¾´é‡ã®çµ±è¨ˆ
        feature_stats = {
            'total_features': len(feature_cols),
            'nan_ratio': np.isnan(X).sum() / X.size,
            'inf_ratio': np.isinf(X).sum() / X.size,
            'zero_variance_features': np.sum(np.var(X, axis=0) == 0),
            'correlation_with_target': self._calculate_feature_correlations(X, y, feature_cols)
        }
        
        # äºˆæ¸¬æ€§èƒ½åˆ†æ
        performance_analysis = self._analyze_prediction_performance(X, y, period_name)
        
        # å•é¡Œç‰¹å®š
        issues = self._identify_issues(basic_stats, feature_stats, performance_analysis)
        
        return {
            'period': period_name,
            'current_accuracy': self.current_accuracies.get(period_name, 0),
            'target_accuracy': 0.50,
            'improvement_needed': 0.50 - self.current_accuracies.get(period_name, 0),
            'basic_stats': basic_stats,
            'feature_stats': feature_stats,
            'performance_analysis': performance_analysis,
            'identified_issues': issues,
            'recommendations': self._generate_recommendations(issues)
        }
    
    def _calculate_feature_correlations(self, X, y, feature_names):
        """ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ç›¸é–¢åˆ†æ"""
        correlations = []
        
        for i, feature_name in enumerate(feature_names):
            try:
                correlation = np.corrcoef(X[:, i], y)[0, 1]
                if not np.isnan(correlation):
                    correlations.append({
                        'feature': feature_name,
                        'correlation': abs(correlation),
                        'raw_correlation': correlation
                    })
            except:
                continue
        
        # ç›¸é–¢ã®é«˜ã„ç‰¹å¾´é‡ãƒˆãƒƒãƒ—10
        correlations.sort(key=lambda x: x['correlation'], reverse=True)
        
        return {
            'top_10_features': correlations[:10],
            'avg_correlation': np.mean([c['correlation'] for c in correlations]) if correlations else 0,
            'high_correlation_count': len([c for c in correlations if c['correlation'] > 0.1])
        }
    
    def _analyze_prediction_performance(self, X, y, period_name):
        """äºˆæ¸¬æ€§èƒ½ã®è©³ç´°åˆ†æ"""
        from sklearn.model_selection import cross_val_score
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.preprocessing import RobustScaler
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        split_idx = int(0.7 * len(X))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = RobustScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # å˜ç´”ãªRFãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½ç¢ºèª
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        
        try:
            # äº¤å·®æ¤œè¨¼
            cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv=3, scoring='neg_mean_squared_error')
            
            # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆ
            rf.fit(X_train_scaled, y_train)
            train_pred = rf.predict(X_train_scaled)
            test_pred = rf.predict(X_test_scaled)
            
            # æ–¹å‘æ€§ç²¾åº¦
            train_direction_acc = self._calculate_direction_accuracy(y_train, train_pred)
            test_direction_acc = self._calculate_direction_accuracy(y_test, test_pred)
            
            # ç‰¹å¾´é‡é‡è¦åº¦
            feature_importance = rf.feature_importances_
            
            return {
                'cv_mse': -cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'train_direction_accuracy': train_direction_acc,
                'test_direction_accuracy': test_direction_acc,
                'overfitting_indicator': train_direction_acc - test_direction_acc,
                'prediction_variance': np.var(test_pred),
                'target_variance': np.var(y_test),
                'top_feature_importance': np.argsort(feature_importance)[-10:][::-1].tolist()
            }
            
        except Exception as e:
            logger.error(f"æ€§èƒ½åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}
    
    def _calculate_direction_accuracy(self, y_true, y_pred):
        """æ–¹å‘æ€§ç²¾åº¦è¨ˆç®—"""
        true_direction = np.sign(y_true)
        pred_direction = np.sign(y_pred)
        
        mask = true_direction != 0
        if np.sum(mask) == 0:
            return 0.5
        
        true_direction = true_direction[mask]
        pred_direction = pred_direction[mask]
        
        return np.mean(true_direction == pred_direction)
    
    def _identify_issues(self, basic_stats, feature_stats, performance_analysis):
        """å•é¡Œç‚¹ã®ç‰¹å®š"""
        issues = []
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªã®å•é¡Œ
        if basic_stats['data_loss_ratio'] > 0.5:
            issues.append({
                'type': 'data_quality',
                'severity': 'high',
                'description': f"ç‰¹å¾´é‡ä½œæˆæ™‚ã®ãƒ‡ãƒ¼ã‚¿æå¤±ãŒå¤§ãã„ ({basic_stats['data_loss_ratio']:.1%})",
                'impact': 'ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³'
            })
        
        # ç‰¹å¾´é‡ã®å•é¡Œ
        if feature_stats['zero_variance_features'] > 5:
            issues.append({
                'type': 'feature_quality',
                'severity': 'medium',
                'description': f"åˆ†æ•£ãŒã‚¼ãƒ­ã®ç‰¹å¾´é‡ãŒå¤šã„ ({feature_stats['zero_variance_features']}å€‹)",
                'impact': 'æƒ…å ±ä¾¡å€¤ã®ä½ã„ç‰¹å¾´é‡'
            })
        
        # ç›¸é–¢ã®å•é¡Œ
        if feature_stats['correlation_with_target']['avg_correlation'] < 0.05:
            issues.append({
                'type': 'feature_relevance',
                'severity': 'high',
                'description': f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã®å¹³å‡ç›¸é–¢ãŒä½ã„ ({feature_stats['correlation_with_target']['avg_correlation']:.3f})",
                'impact': 'äºˆæ¸¬åŠ›ä¸è¶³'
            })
        
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
        if 'overfitting_indicator' in performance_analysis and performance_analysis['overfitting_indicator'] > 0.1:
            issues.append({
                'type': 'overfitting',
                'severity': 'high',
                'description': f"ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã®å…†å€™ (å·®: {performance_analysis['overfitting_indicator']:.3f})",
                'impact': 'æ±åŒ–æ€§èƒ½ã®ä½ä¸‹'
            })
        
        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã®å•é¡Œ
        if basic_stats['price_volatility'] > 0.05:
            issues.append({
                'type': 'market_volatility',
                'severity': 'medium',
                'description': f"é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æœŸé–“ ({basic_stats['price_volatility']:.3f})",
                'impact': 'äºˆæ¸¬å›°é›£ãªå¸‚å ´çŠ¶æ³'
            })
        
        return issues
    
    def _generate_recommendations(self, issues):
        """æ”¹å–„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        for issue in issues:
            if issue['type'] == 'data_quality':
                recommendations.append({
                    'priority': 'high',
                    'action': 'ç‰¹å¾´é‡ä½œæˆãƒ—ãƒ­ã‚»ã‚¹ã®æœ€é©åŒ–',
                    'details': [
                        'æ¬ æå€¤è£œå®Œæ‰‹æ³•ã®æ”¹è‰¯',
                        'ã‚ˆã‚ŠçŸ­æœŸé–“ã®ç§»å‹•å¹³å‡ä½¿ç”¨',
                        'å‰æ–¹/å¾Œæ–¹è£œå®Œã®æ´»ç”¨'
                    ]
                })
            
            elif issue['type'] == 'feature_relevance':
                recommendations.append({
                    'priority': 'high',
                    'action': 'ç‰¹å¾´é‡é¸æŠã®æ”¹è‰¯',
                    'details': [
                        'ç›¸äº’æƒ…å ±é‡ãƒ™ãƒ¼ã‚¹ã®é¸æŠå¼·åŒ–',
                        'ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«ã‚ˆã‚‹ç‰¹å¾´é‡è¿½åŠ ',
                        'éç·šå½¢ç‰¹å¾´é‡ã®å°å…¥'
                    ]
                })
            
            elif issue['type'] == 'overfitting':
                recommendations.append({
                    'priority': 'high',
                    'action': 'æ­£å‰‡åŒ–ã®å¼·åŒ–',
                    'details': [
                        'ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®èª¿æ•´',
                        'ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–',
                        'ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå°å…¥'
                    ]
                })
            
            elif issue['type'] == 'market_volatility':
                recommendations.append({
                    'priority': 'medium',
                    'action': 'ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£å¯¾å¿œ',
                    'details': [
                        'ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£èª¿æ•´ã•ã‚ŒãŸç‰¹å¾´é‡',
                        'å¸‚å ´ãƒ¬ã‚¸ãƒ¼ãƒ åˆ¥ãƒ¢ãƒ‡ãƒ«',
                        'ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–äºˆæ¸¬çª“'
                    ]
                })
        
        # åŸºæœ¬çš„ãªæ”¹å–„ç­–
        recommendations.append({
            'priority': 'medium',
            'action': 'ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–',
            'details': [
                'Bayesian Optimization',
                'Grid Searchå¼·åŒ–',
                'ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿èª¿æ•´'
            ]
        })
        
        return recommendations
    
    def generate_improvement_plan(self, analysis_results):
        """æ”¹å–„è¨ˆç”»ã®ç”Ÿæˆ"""
        logger.info("æ”¹å–„è¨ˆç”»ã‚’ç”Ÿæˆä¸­...")
        
        plan = {
            'summary': {
                'target_periods': len(self.target_periods),
                'current_avg_accuracy': np.mean(list(self.current_accuracies.values())),
                'target_accuracy': 0.50,
                'total_improvement_needed': sum([0.50 - acc for acc in self.current_accuracies.values()])
            },
            'period_priorities': [],
            'technical_actions': [],
            'implementation_order': []
        }
        
        # æœŸé–“åˆ¥å„ªå…ˆåº¦
        for period, result in analysis_results.items():
            improvement_needed = result['improvement_needed']
            issue_severity = len([i for i in result['identified_issues'] if i['severity'] == 'high'])
            
            plan['period_priorities'].append({
                'period': period,
                'current_accuracy': result['current_accuracy'],
                'improvement_needed': improvement_needed,
                'issue_count': len(result['identified_issues']),
                'priority_score': improvement_needed + (issue_severity * 0.01)
            })
        
        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        plan['period_priorities'].sort(key=lambda x: x['priority_score'], reverse=True)
        
        # æŠ€è¡“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        all_issues = []
        for result in analysis_results.values():
            all_issues.extend(result['identified_issues'])
        
        # å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥ã®çµ±è¨ˆ
        issue_types = {}
        for issue in all_issues:
            issue_type = issue['type']
            if issue_type not in issue_types:
                issue_types[issue_type] = 0
            issue_types[issue_type] += 1
        
        # æœ€ã‚‚é »å‡ºã™ã‚‹å•é¡Œã‹ã‚‰å¯¾å‡¦
        for issue_type, count in sorted(issue_types.items(), key=lambda x: x[1], reverse=True):
            plan['technical_actions'].append({
                'issue_type': issue_type,
                'frequency': count,
                'priority': 'high' if count >= 2 else 'medium'
            })
        
        # å®Ÿè£…é †åº
        plan['implementation_order'] = [
            "1. ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šï¼ˆæ¬ æå€¤å‡¦ç†æ”¹å–„ï¼‰",
            "2. ç‰¹å¾´é‡é¸æŠã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ”¹è‰¯", 
            "3. æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´",
            "4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿æœ€é©åŒ–",
            "5. æœŸé–“åˆ¥ãƒ¢ãƒ‡ãƒ«å¾®èª¿æ•´"
        ]
        
        return plan
    
    def save_results(self, analysis_results, improvement_plan):
        """çµæœä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONä¿å­˜
        with open(self.results_dir / f'analysis_results_{timestamp}.json', 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False, default=str)
        
        with open(self.results_dir / f'improvement_plan_{timestamp}.json', 'w', encoding='utf-8') as f:
            json.dump(improvement_plan, f, indent=2, ensure_ascii=False, default=str)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._generate_report(analysis_results, improvement_plan)
        
        logger.info(f"çµæœä¿å­˜å®Œäº†: {self.results_dir}")
    
    def _generate_report(self, analysis_results, improvement_plan):
        """æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""
# äºˆæ¸¬ç²¾åº¦æ”¹å–„åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## æ¦‚è¦

### ç›®æ¨™
- å…¨æœŸé–“ã§äºˆæ¸¬ç²¾åº¦50%ä»¥ä¸Šé”æˆ
- ç¾åœ¨æœªé”æˆ: {len(self.target_periods)}æœŸé–“

### ç¾åœ¨ã®çŠ¶æ³
"""
        
        for period in self.target_periods:
            acc = self.current_accuracies[period]
            needed = 0.50 - acc
            report += f"- {period}: {acc:.2%} (æ”¹å–„å¿…è¦: +{needed:.2%})\n"
        
        report += f"""
### å„ªå…ˆåº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
"""
        
        for i, period_priority in enumerate(improvement_plan['period_priorities'], 1):
            period = period_priority['period']
            score = period_priority['priority_score']
            report += f"{i}. {period} (å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢: {score:.3f})\n"
        
        report += f"""

## æœŸé–“åˆ¥åˆ†æçµæœ

"""
        
        for period, result in analysis_results.items():
            report += f"### {period} æœŸé–“\n\n"
            report += f"- ç¾åœ¨ç²¾åº¦: {result['current_accuracy']:.2%}\n"
            report += f"- æ”¹å–„å¿…è¦: +{result['improvement_needed']:.2%}\n"
            report += f"- ãƒ‡ãƒ¼ã‚¿æ•°: {result['basic_stats']['data_points']}\n"
            report += f"- ç‰¹å¾´é‡æ•°: {result['feature_stats']['total_features']}\n"
            
            report += f"\n**ç‰¹å®šã•ã‚ŒãŸå•é¡Œ:**\n"
            for issue in result['identified_issues']:
                report += f"- [{issue['severity'].upper()}] {issue['description']}\n"
            
            report += f"\n**æ¨å¥¨æ”¹å–„ç­–:**\n"
            for rec in result['recommendations']:
                report += f"- [{rec['priority'].upper()}] {rec['action']}\n"
                for detail in rec['details']:
                    report += f"  - {detail}\n"
            report += "\n"
        
        report += f"""

## å®Ÿè£…è¨ˆç”»

### æŠ€è¡“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå„ªå…ˆé †ï¼‰
"""
        
        for action in improvement_plan['technical_actions']:
            report += f"- {action['issue_type']} (é »åº¦: {action['frequency']}, å„ªå…ˆåº¦: {action['priority']})\n"
        
        report += f"""

### å®Ÿè£…é †åº
"""
        
        for order in improvement_plan['implementation_order']:
            report += f"{order}\n"
        
        report += f"""

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **æœ€å„ªå…ˆ**: {improvement_plan['period_priorities'][0]['period']} æœŸé–“ã®æ”¹å–„
2. **æŠ€è¡“æ”¹å–„**: ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šã¨ç‰¹å¾´é‡é¸æŠæ”¹è‰¯
3. **æ¤œè¨¼**: æ”¹å–„å¾Œã®æ€§èƒ½æ¸¬å®š

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*
"""
        
        with open(self.results_dir / 'improvement_report.md', 'w', encoding='utf-8') as f:
            f.write(report)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    analyzer = AccuracyImprovementAnalyzer()
    
    logger.info("ğŸ¯ äºˆæ¸¬ç²¾åº¦æ”¹å–„åˆ†æã‚’é–‹å§‹...")
    
    try:
        # æœªé”æˆæœŸé–“ã®åˆ†æ
        analysis_results = analyzer.analyze_underperforming_periods()
        
        if not analysis_results:
            logger.error("åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # æ”¹å–„è¨ˆç”»ç”Ÿæˆ
        improvement_plan = analyzer.generate_improvement_plan(analysis_results)
        
        # çµæœä¿å­˜
        analyzer.save_results(analysis_results, improvement_plan)
        
        # çµæœè¡¨ç¤º
        print(f"\n{'='*60}")
        print("äºˆæ¸¬ç²¾åº¦æ”¹å–„åˆ†æ å®Œäº†")
        print(f"{'='*60}")
        
        print(f"\nåˆ†æå¯¾è±¡æœŸé–“: {len(analysis_results)}")
        for period, result in analysis_results.items():
            improvement = result['improvement_needed']
            print(f"  {period}: {result['current_accuracy']:.2%} â†’ 50.0% (+{improvement:.2%})")
        
        print(f"\næœ€å„ªå…ˆæ”¹å–„æœŸé–“: {improvement_plan['period_priorities'][0]['period']}")
        
        print(f"\nä¸»è¦ãªå•é¡Œ:")
        for action in improvement_plan['technical_actions'][:3]:
            print(f"  - {action['issue_type']} (é »åº¦: {action['frequency']})")
        
        print(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: results/accuracy_improvement/improvement_report.md")
        
    except Exception as e:
        logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()